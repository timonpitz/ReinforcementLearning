{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install gym\n",
    "#!pip install matplotlib\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_buckets = np.linspace(-1.2, 0.6, 20)\n",
    "vel_buckets = np.linspace(-0.07, 0.07, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDiscreteStates(obs):\n",
    "    pos, vel = obs\n",
    "    pos_disc = np.digitize(pos, pos_buckets)\n",
    "    vel_disc = np.digitize(vel, vel_buckets)\n",
    "    \n",
    "    return(pos_disc, vel_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_action(Q, obs, actions=[0,1,2]):\n",
    "    state = toDiscreteStates(obs)\n",
    "    values = []\n",
    "    for a in actions:\n",
    "        values.append(Q[state, a])\n",
    "    \n",
    "    action = np.argmax(values)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env._max_episode_steps = 1000\n",
    "n_games = 50000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "eps = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for pos in range(21):\n",
    "    for vel in range(21):\n",
    "        states.append((pos,vel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "for state in states:\n",
    "    for action in [0,1,2]:\n",
    "        Q[state, action] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveQ():\n",
    "    with open('obj/Qtable.pkl', 'wb') as f:\n",
    "        pickle.dump(Q, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadQ(obs, action, reward, new_obs_new_action):\n",
    "    with open('obj/Qtable.pkl', 'rb') as f:\n",
    "        pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQ(obs, action, reward, new_obs, new_action):\n",
    "    state = toDiscreteStates(obs)\n",
    "    new_state = toDiscreteStates(new_obs)\n",
    "    Q[state, action] = Q[state, action] + alpha*(reward + gamma*Q[new_state, new_action] - Q[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results():\n",
    "    Q = loadQ()\n",
    "    new_eps = 0.01\n",
    "    for i in range(10):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            if np.random.random() < new_eps:\n",
    "                action = np.random.choice ([0,1,2])\n",
    "            else:\n",
    "                action = max_action(Q, obs)\n",
    "                \n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            new_action = max_action(Q, new_obs)\n",
    "            updateQ(obs, action, reward, new_obs, new_action)\n",
    "            obs = new_obs\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  100 score  -1000.0 eps  0.995999999999996\n",
      "episode  200 score  -1000.0 eps  0.991999999999992\n",
      "episode  300 score  -1000.0 eps  0.987999999999988\n",
      "episode  400 score  -948.0 eps  0.983999999999984\n",
      "episode  500 score  -1000.0 eps  0.97999999999998\n",
      "episode  600 score  -1000.0 eps  0.975999999999976\n",
      "episode  700 score  -1000.0 eps  0.971999999999972\n",
      "episode  800 score  -1000.0 eps  0.967999999999968\n",
      "episode  900 score  -1000.0 eps  0.963999999999964\n",
      "episode  1000 score  -1000.0 eps  0.95999999999996\n",
      "episode  1100 score  -1000.0 eps  0.955999999999956\n",
      "episode  1200 score  -1000.0 eps  0.951999999999952\n",
      "episode  1300 score  -1000.0 eps  0.947999999999948\n",
      "episode  1400 score  -1000.0 eps  0.943999999999944\n",
      "episode  1500 score  -1000.0 eps  0.93999999999994\n",
      "episode  1600 score  -1000.0 eps  0.935999999999936\n",
      "episode  1700 score  -1000.0 eps  0.931999999999932\n",
      "episode  1800 score  -1000.0 eps  0.927999999999928\n",
      "episode  1900 score  -724.0 eps  0.923999999999924\n",
      "episode  2000 score  -1000.0 eps  0.91999999999992\n",
      "episode  2100 score  -1000.0 eps  0.915999999999916\n",
      "episode  2200 score  -1000.0 eps  0.911999999999912\n",
      "episode  2300 score  -1000.0 eps  0.907999999999908\n",
      "episode  2400 score  -1000.0 eps  0.903999999999904\n",
      "episode  2500 score  -1000.0 eps  0.8999999999999\n",
      "episode  2600 score  -1000.0 eps  0.895999999999896\n",
      "episode  2700 score  -751.0 eps  0.891999999999892\n",
      "episode  2800 score  -1000.0 eps  0.887999999999888\n",
      "episode  2900 score  -1000.0 eps  0.883999999999884\n",
      "episode  3000 score  -1000.0 eps  0.87999999999988\n",
      "episode  3100 score  -1000.0 eps  0.875999999999876\n",
      "episode  3200 score  -1000.0 eps  0.871999999999872\n",
      "episode  3300 score  -1000.0 eps  0.867999999999868\n",
      "episode  3400 score  -1000.0 eps  0.863999999999864\n",
      "episode  3500 score  -1000.0 eps  0.85999999999986\n",
      "episode  3600 score  -1000.0 eps  0.855999999999856\n",
      "episode  3700 score  -1000.0 eps  0.851999999999852\n",
      "episode  3800 score  -581.0 eps  0.847999999999848\n",
      "episode  3900 score  -1000.0 eps  0.843999999999844\n",
      "episode  4000 score  -1000.0 eps  0.83999999999984\n",
      "episode  4100 score  -855.0 eps  0.835999999999836\n",
      "episode  4200 score  -814.0 eps  0.831999999999832\n",
      "episode  4300 score  -1000.0 eps  0.827999999999828\n",
      "episode  4400 score  -1000.0 eps  0.823999999999824\n",
      "episode  4500 score  -1000.0 eps  0.81999999999982\n",
      "episode  4600 score  -713.0 eps  0.815999999999816\n",
      "episode  4700 score  -1000.0 eps  0.811999999999812\n",
      "episode  4800 score  -651.0 eps  0.807999999999808\n",
      "episode  4900 score  -1000.0 eps  0.803999999999804\n",
      "episode  5000 score  -1000.0 eps  0.7999999999998\n",
      "episode  5100 score  -1000.0 eps  0.795999999999796\n",
      "episode  5200 score  -1000.0 eps  0.791999999999792\n",
      "episode  5300 score  -1000.0 eps  0.787999999999788\n",
      "episode  5400 score  -1000.0 eps  0.783999999999784\n",
      "episode  5500 score  -772.0 eps  0.77999999999978\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "total_rewards = np.zeros(n_games)\n",
    "\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    if i % 100 == 0 and i>0:\n",
    "        print('episode ', i, 'score ', score, 'eps ', eps)\n",
    "    score = 0\n",
    "    while not done:\n",
    "        if np.random.random() < eps:\n",
    "            action = np.random.choice([0,1,2])\n",
    "        else:\n",
    "            action = max_action(Q, obs)\n",
    "        \n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        new_action = max_action(Q, new_obs)\n",
    "        updateQ(obs, action, reward, new_obs, new_action)\n",
    "        obs = new_obs\n",
    "    total_rewards[i] = score\n",
    "    if eps > 0.01:\n",
    "        eps = eps - 2/n_games\n",
    "    else:\n",
    "        eps = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveQ()\n",
    "show_result()\n",
    "mean_rewards = np.zeros(n_games)\n",
    "for r in range(n_games):\n",
    "    mean_rewards[r] = np.mean(total_rewards[max(0, r-50):(r+1)])\n",
    "plt.plot(mean_rewards)\n",
    "plt.savefig('mountaincar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
